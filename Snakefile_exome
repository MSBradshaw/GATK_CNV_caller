############################################################
#SV-EXOME, a snakefile for calling CNVs in exome with GATK4#
############################################################

import os
import json
import glob

#Get PATHS from config file
workdir: config['SV-EXOME_DIR']
OUTPUT_DIR = config['OUTPUT_DIR']

# Get paths of required reference files
REFERENCEGENOME = config["HG_PATH"]
HGDICT = config["DICT_PATH"]
BED = config["BED_PATH"]

SAMPLES_DIR = config["SAMPLES_PATH"]
SAMPLE_WC = config["SAMPLE_WILDCARD"]
SAMPLES, = glob_wildcards(SAMPLES_DIR+"/"+SAMPLE_WC)

SAMPLES_GLOB = glob.glob(OUTPUT_DIR+"/*am")

SAMPLES_NAME = []
# Check if samples have a valid extension and make a list of the names of the samples
for i in SAMPLES_GLOB:
    if not (i.endswith(".bam") or i.endswith(".cram")):
        print(i+" has not a valid extension. It should be a .bam or .cram file") 
        exit()
    else:
        SAMPLES_NAME.append(i.split("/")[-1].replace(".cram","").replace(".bam","").split(config["SAMPLE_SEPARATOR"])[int(config["SAMPLE_INDEX"])])

SCATTER = ["temp_0001_of_4","temp_0002_of_4","temp_0003_of_4","temp_0004_of_4"]

CHROMOSOME = ["chr1", "chr2", "chr3", "chr4", "chr5", "chr6", "chr7", "chr8", "chr9", "chr10", "chr11", "chr12", "chr13", "chr14", "chr15", "chr16", "chr17", "chr18", "chr19", "chr20", "chr21", "chr22", "chrX", "chrY"]



### RULES ### 

rule target:
    input: expand(OUTPUT_DIR+"/count.{sample}.tsv", sample=SAMPLES_NAME,), expand(OUTPUT_DIR+"/scatter/{scatter}/scattered.interval_list", scatter = SCATTER,), OUTPUT_DIR+"/ploidy-calls_chr", expand(OUTPUT_DIR+"/{sample}.segments.vcf.gz", sample=SAMPLES_NAME,), OUTPUT_DIR+"/database_sorted.tab", expand(OUTPUT_DIR+"/{sample}.segments_filtered.vcf.gz", sample=SAMPLES_NAME,), expand(OUTPUT_DIR+"/{sample}.segments_filtered.vcf.gz", sample=SAMPLES_NAME,)

rule MAKE_INTERVALS:
    input: ref = REFERENCEGENOME, bed = BED 
    output: OUTPUT_DIR+"/interval_whole_exome.interval_list"
    conda : "envs/gatkcondaenv.yml"
    shell:
        """
        gatk --java-options "-Xmx{config[JAVA_ARGS][MAX_MEMORY]} -Djava.io.tmpdir={config[TEMP_DIR]}" PreprocessIntervals \
        -R {input.ref} \
	    -L {input.bed} \
        --interval-merging-rule OVERLAPPING_ONLY \
        -O {output}
        """

def get_name(wildcards):
    for i in SAMPLES_GLOB:
       if list({wildcards.sample})[0] in i:
            return { "ALIGN" : i }
    
rule COLLECT_READCOUNT:
    input: unpack(get_name)
    params: ref = REFERENCEGENOME, interval = OUTPUT_DIR+"/interval_whole_exome.interval_list"
    output: samp = OUTPUT_DIR+"/count.{sample}.tsv"
    conda : "envs/gatkcondaenv.yml"
    shell: """
	gatk --java-options "-Xmx{config[JAVA_ARGS][MAX_MEMORY]} -Djava.io.tmpdir={config[TEMP_DIR]}" CollectReadCounts --disable-read-filter WellformedReadFilter -R {params.ref} -I {input.ALIGN} -L {params.interval} -imr OVERLAPPING_ONLY --format TSV -O {output}
	"""

rule ANNOTATE_INTERVALS:
    input: interval = OUTPUT_DIR+"/interval_whole_exome.interval_list", ref = REFERENCEGENOME
    output: OUTPUT_DIR+"/whole_exome_annotated_intervals.tsv"
    conda : "envs/gatkcondaenv.yml"
    shell:"""
    gatk --java-options "-Xmx{config[JAVA_ARGS][MAX_MEMORY]} -Djava.io.tmpdir={config[TEMP_DIR]}" AnnotateIntervals -L {input.interval} -R {input.ref} -imr OVERLAPPING_ONLY -O {output}
    """


rule FILTER_INTERVALS:
    input: interval = OUTPUT_DIR+"/interval_whole_exome.interval_list", count = expand(OUTPUT_DIR+"/count.{sample}.tsv", sample=SAMPLES_NAME,), annotated = OUTPUT_DIR+"/whole_exome_annotated_intervals.tsv"
    output: OUTPUT_DIR+"/whole_exome_cohort_gccontent.filtered.interval_list"
    conda : "envs/gatkcondaenv.yml"
    shell:"""
	a=""
	for i in {input.count}; do a=$a" -I "$i; done
	gatk --java-options "-Xmx{config[JAVA_ARGS][MAX_MEMORY]} -Djava.io.tmpdir={config[TEMP_DIR]}" FilterIntervals -L {input.interval} --annotated-intervals {input.annotated} $a -imr OVERLAPPING_ONLY -O {output} 
	"""

rule PLOIDY:
    input: cohort = OUTPUT_DIR+"/whole_exome_cohort_gccontent.filtered.interval_list", priors = "conf/contig_ploidy_priors.tsv", tsv = expand(OUTPUT_DIR+"/count.{sample}.tsv", sample=SAMPLES_NAME,)
    output: directory(OUTPUT_DIR+"/ploidy-calls_chr")
    params: prefix = "ploidy"
    conda: "envs/gatkcondaenv.yml"
    shell: """
	export MKL_NUM_THREADS={config[GATK][MAX_THREADS]}
	export OMP_NUM_THREADS={config[GATK][MAX_THREADS]}
	a=""
	for i in {input.tsv}; do a=$a" -I "$i; done
	gatk --java-options "-Xmx{config[JAVA_ARGS][MAX_MEMORY]} -Djava.io.tmpdir={config[TEMP_DIR]}" DetermineGermlineContigPloidy -L {input.cohort} $a --imr OVERLAPPING_ONLY --contig-ploidy-priors {input.priors} --output {output} --output-prefix {params}
	"""

rule SCATTER:
        input: OUTPUT_DIR+"/whole_exome_cohort_gccontent.filtered.interval_list"
        params: dir = directory(OUTPUT_DIR+"/scatter")
        output: expand(OUTPUT_DIR+"/scatter/{scatter}/scattered.interval_list", scatter=SCATTER,)
        shell: """
        gatk --java-options "-Xmx{config[JAVA_ARGS][MAX_MEMORY]} -Djava.io.tmpdir={config[TEMP_DIR]}"  IntervalListTools --SUBDIVISION_MODE BALANCING_WITHOUT_INTERVAL_SUBDIVISION --SCATTER_COUNT 4 --INPUT {input} --OUTPUT {params.dir}
        """

rule GERMLINE_CNV_CALLER:
    input: interval = OUTPUT_DIR+"/scatter/{fragment}/scattered.interval_list", tsv = expand(OUTPUT_DIR+"/count.{sample}.tsv", sample=SAMPLES_NAME,), priors = "conf/contig_ploidy_priors.tsv", anno = OUTPUT_DIR+"/whole_exome_annotated_intervals.tsv", ploidy=OUTPUT_DIR+"/ploidy-calls_chr"
    params: prefix="fragment_{fragment}", outdir=OUTPUT_DIR+"/cohort_germline_calling"
    output: model=directory(OUTPUT_DIR+"/cohort_germline_calling/{fragment}-model"), call=directory(OUTPUT_DIR+"/cohort_germline_calling/{fragment}-calls")
    conda: "envs/gatkcondaenv.yml"
    shell: """
        export MKL_NUM_THREADS=12
        export OMP_NUM_THREADS=12
        a=""
        for i in {input.tsv}; do a=$a" -I "$i; done
        gatk --java-options "-Xmx{config[JAVA_ARGS][MAX_MEMORY]} -Djava.io.tmpdir={config[TEMP_DIR]}" GermlineCNVCaller \
        -L {input.interval} \
        --run-mode COHORT \
        $a \
        --contig-ploidy-calls {input.ploidy}/ploidy-calls \
        --annotated-intervals {input.anno} \
        --interval-merging-rule OVERLAPPING_ONLY \
        --output {params.outdir} \
        --output-prefix {params.prefix} \
        --verbosity DEBUG
        """


def index(sample):
    for i in glob.glob(OUTPUT_DIR+"/cohort_germline_calling/whole_exome-calls/SAMPLE_*/sample_name.txt"):
        file=open(i,'r')
        for line in file:
           if line.rstrip() == sample: 
               return i.split('/')[-2].split('_')[1]

rule POSTPROCESS:
        input: sam = OUTPUT_DIR+"/count.{sample}.tsv", model=expand(OUTPUT_DIR+"/cohort_germline_calling/{scatter}-model", scatter = SCATTER,), calls=expand(OUTPUT_DIR+"/cohort_germline_calling/{scatter}-calls", scatter = SCATTER,)
        output: int = OUTPUT_DIR+"/{sample}.intervals.vcf.gz", seg = OUTPUT_DIR+"/{sample}.segments.vcf.gz", den = OUTPUT_DIR+"/{sample}.denoise"
        params: id = lambda wildcards: index(wildcards.sample), ploidy_path = OUTPUT_DIR+"/ploidy-calls_chr/ploidy-calls/", dict = HGDICT,
                modelfiles = lambda wildcards, input: " --model-shard-path ".join(input.model),
                callsfiles = lambda wildcards, input: " --calls-shard-path ".join(input.calls),
        conda: "envs/gatkcondaenv.yml"
        shell: """
        export MKL_NUM_THREADS=1
        export OMP_NUM_THREADS=1
        gatk --java-options "-Xmx{config[JAVA_ARGS][MAX_MEMORY]} -Djava.io.tmpdir={config[TEMP_DIR]}" PostprocessGermlineCNVCalls \
        --model-shard-path {params.modelfiles} \
        --calls-shard-path {params.callsfiles} \
        --allosomal-contig chrX --allosomal-contig chrY \
        --contig-ploidy-calls {params.ploidy_path} \
        --sample-index {params.id} \
        --output-genotyped-intervals {output.int} \
        --output-genotyped-segments {output.seg} \
        --output-denoised-copy-ratios {output.den} \
        --sequence-dictionary {params.dict}
        """
	
rule DATABASE:
	input: expand(OUTPUT_DIR+"/{sample}.segments.vcf.gz", sample=SAMPLES_NAME,)
	output: OUTPUT_DIR+"/database.tab"
	script: "Scripts/gatk4_cnv_database_file_snakemake.py"
		
rule BED_SORT:
	input: OUTPUT_DIR+"/database.tab"
	output: OUTPUT_DIR+"/database_sorted.tab"
	shell: """
	echo -e "#CHR\tSTART\tSTOP\tCNV\tNUMBER\tSAMPLES" > {output}
	sort -V -k1,1 -k 2,2 {input} >> {output}
	"""

rule FILTER_VCF:
	input: database=rules.BED_SORT.output, samples=OUTPUT_DIR+"/{sample}.segments.vcf.gz"
	output: OUTPUT_DIR+"/{sample}.segments_filtered.vcf.gz"
	script: "Scripts/VCF_CNV_GATK4_denovo_annotate_filter.py" 

#rule ANNOT_VCF:
#	input: OUTPUT_DIR+"/{sample}.segments_filtered.vcf.gz"
#	output: OUTPUT_DIR+"/{sample}.segments_filtered_anno.tab"
#	conda: "python_mysql.yml"
#	script: "Scripts/Script_annotation_GATK4_exome_CNV_1.0_standalone.py"

#rule MERGE_CNVS:
#	input: expand(OUTPUT_DIR+"/{sample}.segments_filtered_anno.tab", sample=SAMPLES_NAME,)
#	output: OUTPUT_DIR+"/all_cnv_calls_anno.tab"
#shell: """
#a=0;
#for i in {input}; do 
#a=$((a+1))
#echo $a
#if [ $a -eq "1" ];
#then 
#	cat $i > {output}
#else
#	tail -n +2 $i >> {output}
#fi
#done
#"""
